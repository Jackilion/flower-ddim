{"cells":[{"cell_type":"markdown","source":["\n","# Denoising Diffusion Implicit Model\n","In diesem Notebook soll es darum gehen, die Grundlagen der Technologie von den zurzeit viel bepriesenen Image Generator Modellen wie StableDiffusion zu verstehen.\n","\n","Jeder Code Block kann durch klicken des \"Play\" Buttons oben Links ausgeführt werden. Es ist wichtig die Blöcke in der richtigen Reihenfolge auszuführen, ansonsten kann es zu Fehlern kommen.\n","\n","\n"],"metadata":{"id":"YtVdTF4jbRRa"}},{"cell_type":"markdown","source":["##Import und Setup\n","\n","Jedes Programm startet mit sogenannten Imports. Hierbei handelt es sich um das Laden von anderen, bereits programmierten Scripts.\n","In unserem Fall benutzen wir TensorFlow, ein Framework von Google, welches dazu dient Machine Learning Modelle zu programmieren.\n","\n"],"metadata":{"id":"1a2KlmRsb7Ek"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pu93o9wiFM7y"},"outputs":[],"source":["import tensorflow as tf #Dieses Package definiert die eigentlichen neuronalen Netzstrukturen, sowie Datentypen und Rechenoperationen um sie zu manipulieren\n","import tensorflow_datasets as tfds #Dieses Package dient dem Download von Datensets, damit wir sie nicht per Hand downloaden müssen\n","import matplotlib.pyplot as plt #Ein Package, welches dem plotten von Daten dient. Wir benutzen es um die Bilder aus dem Datenset zu plotten, sowie unsere Trainingsergebnisse\n","import keras #Ein High Level Framework welches auf TensorFlow aufbaut. Hat viele nützliche Funktionen die uns das Leben erleichtern"]},{"cell_type":"markdown","source":["Außerdem brauchen wir 2 scripts, die das eigentliche DDIM implementieren. Dessen verständnis ist nicht notwendig, falls du neugierig bist und etwas python verstehst, kannst du aber gerne den Source code anschauen. Klappe einfach die folgende section aus, klicke auf \"play\" und klappe sie wieder zu."],"metadata":{"id":"NTrNRBSxgHwj"}},{"cell_type":"markdown","source":["### Diffusion Model"],"metadata":{"id":"RkN7XhBagVd6"}},{"cell_type":"code","source":["import tensorflow as tf\n","import keras\n","from keras import layers\n","import math\n","import matplotlib.pyplot as plt\n","\n","\n","embedding_dims = 32\n","embedding_max_frequency = 1000.0\n","embedding_min_frequency = 1.0\n","\n","\n","def sinusoidal_embedding(x):\n","    frequencies = tf.exp(\n","        tf.linspace(\n","            tf.math.log(embedding_min_frequency),\n","            tf.math.log(embedding_max_frequency),\n","            embedding_dims // 2,\n","        )\n","    )\n","    angular_speeds = 2.0 * math.pi * frequencies\n","    embeddings = tf.concat(\n","        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n","    )\n","    return embeddings\n","\n","\n","def ResidualBlock(width):\n","    def apply(x):\n","        input_width = x.shape[3]\n","        if input_width == width:\n","            residual = x\n","        else:\n","            residual = layers.Conv2D(width, kernel_size=1)(x)\n","        x = layers.BatchNormalization(center=False, scale=False)(x)\n","        x = layers.Conv2D(\n","            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n","        )(x)\n","        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n","        x = layers.Add()([x, residual])\n","        return x\n","\n","    return apply\n","\n","\n","def DownBlock(width, block_depth):\n","    def apply(x):\n","        x, skips = x\n","        for _ in range(block_depth):\n","            x = ResidualBlock(width)(x)\n","            skips.append(x)\n","        x = layers.AveragePooling2D(pool_size=2)(x)\n","        return x\n","\n","    return apply\n","\n","\n","def UpBlock(width, block_depth):\n","    def apply(x):\n","        x, skips = x\n","        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n","        for _ in range(block_depth):\n","            x = layers.Concatenate()([x, skips.pop()])\n","            x = ResidualBlock(width)(x)\n","        return x\n","\n","    return apply\n","\n","\n","def get_network(image_size, widths, block_depth):\n","    noisy_images = keras.Input(shape=(image_size, image_size, 3))\n","    noise_variances = keras.Input(shape=(1, 1, 1))\n","\n","    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n","    e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)\n","\n","    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n","    x = layers.Concatenate()([x, e])\n","\n","    skips = []\n","    for width in widths[:-1]:\n","        x = DownBlock(width, block_depth)([x, skips])\n","\n","    for _ in range(block_depth):\n","        x = ResidualBlock(widths[-1])(x)\n","\n","    for width in reversed(widths[:-1]):\n","        x = UpBlock(width, block_depth)([x, skips])\n","\n","    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n","\n","    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")\n","\n","\n","\n","\n","max_signal_rate = 0.95\n","min_signal_rate = 0.02\n","ema = 0.999\n","plot_diffusion_steps = 50\n","\n","\n","class DiffusionModel(keras.Model):\n","    def __init__(self, image_size, widths, block_depth, batch_size):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.normalizer = keras.layers.Normalization()\n","        self.image_size = image_size\n","        self.network = get_network(image_size, widths, block_depth)\n","        self.ema_network = keras.models.clone_model(self.network)\n","\n","    def compile(self, **kwargs):\n","        super().compile(**kwargs)\n","        self.noise_loss_tracker = keras.metrics.Mean(name=\"noise_loss\")\n","        self.image_loss_tracker = keras.metrics.Mean(name=\"image_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return[self.noise_loss_tracker, self.image_loss_tracker]\n","\n","    def denormalize(self, images):\n","        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n","        return tf.clip_by_value(images, 0.0, 1.0)\n","\n","    def diffusion_schedule(self, diffusion_times):\n","        # diffusion times -> angles\n","        start_angle = tf.acos(max_signal_rate)\n","        end_angle = tf.acos(min_signal_rate)\n","\n","        diffusion_angles = start_angle + \\\n","            diffusion_times * (end_angle - start_angle)\n","\n","        # angles -> signal and noise rates\n","        signal_rates = tf.cos(diffusion_angles)\n","        noise_rates = tf.sin(diffusion_angles)\n","        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n","\n","        return noise_rates, signal_rates\n","\n","    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n","        # the exponential moving average weights are used at evaluation\n","        if training:\n","            network = self.network\n","        else:\n","            network = self.network\n","\n","        # predict noise component and calculate the image component using it\n","        pred_noises = network(\n","            [noisy_images, noise_rates**2], training=training)\n","\n","        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n","\n","        return pred_noises, pred_images\n","\n","    def reverse_diffusion(self, initial_noise, diffusion_steps):\n","        # reverse diffusion = sampling\n","        num_images = initial_noise.shape[0]\n","        step_size = 1.0 / diffusion_steps\n","\n","        # important line:\n","        # at the first sampling step, the \"noisy image\" is pure noise\n","        # but its signal rate is assumed to be nonzero (min_signal_rate)\n","        next_noisy_images = initial_noise\n","        for step in range(diffusion_steps):\n","            noisy_images = next_noisy_images\n","\n","            # separate the current noisy image to its components\n","            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n","            noise_rates, signal_rates = self.diffusion_schedule(\n","                diffusion_times)\n","            pred_noises, pred_images = self.denoise(\n","                noisy_images, noise_rates, signal_rates, training=False\n","            )\n","            # network used in eval mode\n","\n","            # remix the predicted components using the next signal and noise rates\n","            next_diffusion_times = diffusion_times - step_size\n","            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n","                next_diffusion_times\n","            )\n","            next_noisy_images = (\n","                next_signal_rates * pred_images + next_noise_rates * pred_noises\n","            )\n","            # this new noisy image will be used in the next step\n","\n","        return pred_images\n","\n","    def generate(self, num_images, diffusion_steps):\n","        if self.image_size >= 256:\n","            # Too big, need to generate one by one.\n","            images = []\n","            for i in range(num_images):\n","                initial_noise = tf.random.normal(\n","                    shape=(1, self.image_size, self.image_size, 3))\n","                generated_images = self.reverse_diffusion(\n","                    initial_noise, diffusion_steps)\n","                generated_images = self.denormalize(generated_images)\n","                images.append(tf.squeeze(generated_images))\n","            return images\n","\n","        else:\n","            # noise -> images -> denormalized images\n","            initial_noise = tf.random.normal(\n","                shape=(num_images, self.image_size, self.image_size, 3))\n","            generated_images = self.reverse_diffusion(\n","                initial_noise, diffusion_steps)\n","            generated_images = self.denormalize(generated_images)\n","            return generated_images\n","\n","    def train_step(self, images):\n","        # normalize images to have standard deviation of 1, like the noises\n","        images = self.normalizer(images, training=True)\n","        noises = tf.random.normal(\n","            shape=(self.batch_size, self.image_size, self.image_size, 3))\n","\n","        # sample uniform random diffusion times\n","        diffusion_times = tf.random.uniform(\n","            shape=(self.batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n","        )\n","        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n","        # mix the images with noises accordingly\n","\n","        noisy_images = signal_rates * images + noise_rates * noises\n","\n","        with tf.GradientTape() as tape:\n","            # train the network to separate noisy images to their components\n","            pred_noises, pred_images = self.denoise(\n","                noisy_images, noise_rates, signal_rates, training=True\n","            )\n","\n","            noise_loss = self.loss(noises, pred_noises)  # used for training\n","            image_loss = self.loss(images, pred_images)  # only used as metric\n","\n","        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n","        self.optimizer.apply_gradients(\n","            zip(gradients, self.network.trainable_weights))\n","\n","        self.noise_loss_tracker.update_state(noise_loss)\n","        self.image_loss_tracker.update_state(image_loss)\n","\n","        # track the exponential moving averages of weights\n","        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n","            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n","\n","        # KID is not measured during the training phase for computational efficiency\n","        return {m.name: m.result() for m in self.metrics[:-1]}\n","\n","    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):\n","        # plot random generated images for visual evaluation of generation quality\n","        generated_images = self.generate(\n","            num_images=num_rows * num_cols,\n","            diffusion_steps=plot_diffusion_steps,\n","        )\n","        # print(generated_images)\n","\n","        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n","        for row in range(num_rows):\n","\n","            #image = generated_images[row].numpy()\n","            #plt.subplot(1, 5, row + 1)\n","            #img = Image.fromarray(image, 'RGB')\n","            # plt.imshow(image)\n","            for col in range(num_cols):\n","                index = row * num_cols + col\n","                plt.subplot(num_rows, num_cols, index + 1)\n","                image = generated_images[index].numpy()\n","                plt.imshow(image)\n","                plt.axis(\"off\")\n","                # plt.show()\n","        plt.tight_layout()\n","        plt.show()\n","        plt.savefig(\"epoch_\" + str(epoch))\n","        plt.close()\n","\n"],"metadata":{"id":"VZdM9xkrgdRv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Datenset"],"metadata":{"id":"xntXRtRpgd9M"}},{"cell_type":"markdown","source":["Als nächstes müssen wir unsere trainingsdaten Laden. In diesem Fall benutzen wir das Image set \"Oxford flowers 102\", ein Datenset mit etwa 6000 Bildern von Blüten.\n","\n","Im nächsten code Block wird das Datenset heruntergeladen (was eine weile dauern kann), und ein paar Beispielbilder ausgegeben:"],"metadata":{"id":"URDxETWecplB"}},{"cell_type":"code","source":["dataset = tfds.load(\"oxford_flowers102\", split=\"train[:80%]+validation[:80%]+test[:80%]\", shuffle_files=True)\n","\n","plt.rcParams[\"figure.figsize\"] = [30, 15]\n","plt.rcParams[\"figure.autolayout\"] = True\n","ctr = 0\n","for data in dataset:\n","  image = data[\"image\"]\n","  image = image.numpy()\n","  plt.subplot(1, 5, ctr+1)\n","  plt.title('Label {}'.format(data[\"label\"]))\n","  plt.imshow(image, cmap=plt.cm.binary)\n","  ctr += 1\n","  if ctr == 5:\n","    break"],"metadata":{"id":"zNtNGj_0dFL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSUycgLUFPQp"},"source":["Die Nummer die neben dem Label ausgegeben wird entspricht der \"Klasse\" der dieses Bild angehört. In diesem Fall steht die Nummer für eine bestimmte Pflanzenart. Diese Labels sind wichtig wenn man z.B. ein neuronales Netz bauen möchte, was die Pflanzenart anhand eines Bildes der Blüte erkennt. Dann kann man sie benutzen um dem Netz feedback zu geben ob es richtig oder falsch liegt.\n","\n","Außerdem sind sie wichtig wenn man bestimmte Blüten generieren möchte. Beispielsweise wäre es cool dem Netz sagen zu können man möchte gerne ein Bild einer Sonnenblume haben. Dies ist mit DDIM Models möglich, aber nicht Teil dieses Notebooks. Entwickelt man diese Technik dann weiter, kann man sogar Text-to-Image generation machen, wie z.B. StableDiffusion von OpenAI.\n","\n","\n","\n"]},{"cell_type":"markdown","source":["####<font color=\"blue\">Frage: Was Fällt auf bei den Bildern auf was für unser Netz relevant sein könnte?</font>\n"],"metadata":{"id":"Tzk8bTUFgSYK"}},{"cell_type":"markdown","source":["<font color=\"blue\">*Antwort: Die Bilder haben verschiedene Größen. Das ist ein Problem für neuronale Netzwerke. Denn diese Erwarten einen Input von immer gleicher größe. Schließlich lernen sie exakte operationen auf dem Input durchzuführen. Deshalb müssen wir bevor wir loslegen können die Bilder alle auf die selbe Größe bringen.*</font>"],"metadata":{"id":"6YNfJ7hugWqN"}},{"cell_type":"markdown","source":["## Vorverarbeitung der Daten\n","\n","Dies ist ein wichtiger Teil in Machine Learning. Selbst wenn das neuronale Netz einwandfrei funktioniert, kann es sein dass das Training fehl schlägt, wenn es Probleme mit dem Datenset gibt. Dies beinhaltet das sogenannte normalisieren von Daten, aber auch eine Analyse des Datensets selbst. Gibt es vielleicht einen möglichen bias?\n","\n","Beispiel hier wäre ein oft genutztes Datenset: CelebA\n","Ein Datenset mit Gesichtern von Berühmtheiten. Es wird oft benutzt um facial recognition oder generation zu trainieren. Allerdings hat es einen klaren bias:"],"metadata":{"id":"YiJJDrnghDlN"}},{"cell_type":"markdown","source":["####<font color=\"blue\">Frage: Welchen bias erwartest du in einem Datenset von Gesichtern von berühmten Personen?</font>"],"metadata":{"id":"HylGFQ8uiRB3"}},{"cell_type":"markdown","source":["<font color=\"blue\">*Berühmtheiten haben oft ein sehr symmetrisches Gesicht und sind überwiegend weiß. Dies hat in der Vergangenheit dazu geführt, dass z.B. Facial Login methoden auf Smartphones bei nicht-weißen Personen wesentlich schlechter funktioniert haben.*</font>"],"metadata":{"id":"fqLwZvs2iXSq"}},{"cell_type":"markdown","source":["####<font color=\"blue\">Frage: Welchen bias erwartest du in unserem Datenset?</font>"],"metadata":{"id":"5pBeGOsfjWfD"}},{"cell_type":"markdown","source":["<font color=\"blue\">*Antwort: Nicht jede Pflanzenart ist hier gleich verteten. Viele sogar gar nicht. Außerdem Blühen Pflanzen zu verschiedenen Tages/ Jahreszeiten, was für verschiedene Lichtverhältnisse beim aufnehmen der Photos sorgt.*</font>"],"metadata":{"id":"5HEkr2zgjbHo"}},{"cell_type":"markdown","source":["###Normalisieren der Bilder\n","\n","In unserem Fall geben wir uns mit dem Normalisieren der Bilder zufrieden. Das bedeutet hier, das alle Bilder auf die selben Maße zugeschnitten werden. Wir müssen hier leider auf 64x64 zurückgreifen. Dies ist der Performance geschuldet, denn wir müssen die Trainingszeit stark optimieren um im Rahmen dieses Workshops überhaupt Ergebnisse zu erhalten. Falls du zuhause nochmal rumspielen möchtest, versuche doch mal im nachfolgenden code block die IMG_SIZE Variable auf 128 oder 256 zu setzen.\n","\n","Außerdem werden die Farbkanäle der Bilder auf 1 normiert. Normalerweise werden Farben im RGB-Format (Rot-Gelb-Grün) kodiert, auf einer Skala von 1-255. Ein RGB Wert von 0, 0, 0 bedeutet z.B. weder Rot, noch Gelb, noch Grün ist vorhanden, und die Farbe ist demnach schwarz.\n","Neuronale Netzwerke lieben input der zwischen 0 und 1 liegt. deshalb teilen wir jeden RGB wert durch 255. damit ist z.B. 255, 255, 255 auf 1, 1, 1 geschrumpft. Die Information ist die selbe, aber es erleichtert das Training."],"metadata":{"id":"2nO-qmDrinTf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pnKmbBO8QcHW"},"outputs":[],"source":["IMG_SIZE = 64\n","BATCH_SIZE = 128\n","\n","def format_image(data):\n","    height = tf.shape(data[\"image\"])[0]\n","    width = tf.shape(data[\"image\"])[1]\n","    crop_size = tf.minimum(height, width)\n","    image = tf.image.crop_to_bounding_box(\n","        data[\"image\"],\n","        (height - crop_size) // 2,\n","        (width - crop_size) // 2,\n","        crop_size,\n","        crop_size,\n","    )\n","    image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE], antialias=True)\n","    return tf.clip_by_value(image / 255.0, 0.0, 1.0)\n","\n","\n","training_set = (dataset\n","                   .map(format_image, num_parallel_calls=tf.data.AUTOTUNE)\n","                   .cache()\n","                   .repeat(5)\n","                   .shuffle(10 * BATCH_SIZE)\n","                   .batch(BATCH_SIZE, drop_remainder=True)\n","                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n",")"]},{"cell_type":"markdown","source":["##Definition des Models & Training\n","\n","Nun geht es endlich an das eigentliche Model. Die eigentliche implementierung führt hier ein wenig weit. Bei interesse kannst du gerne einen Blick in die scripts \"diffusion_model.py\", in der der Trainingsprozess definiert ist, sowie \"model.py\", in dem das eigentliche neuronale netz gebaut wird werfen.\n","\n","Für unsere Zwecke reicht ein high-level Überblick über das Netzwerk.\n","Wie in der Einführung erklärt, beruht ein Diffusion Model auf einem UNet.\n","Das bedeutet der Input besitzt die selben dimensionen (in unserem Fall 64x64) wie der output. Im innern des Netzwerks werden dann die dimensionen des Inputs Schritt für Schritt erniedrigt, und dann wieder erhöht. Dies führt dazu, dass das Model lernen muss, wesentliche Informationen in kondensierter Form darzustellen. Ein Mensch merkt sich ein Bild auch nicht Pixel für Pixel. Vielmehr benutzen Menschen abstrakte Aussagen, wie z.B. \"Das Bild zeigt eine schwebende, rote Rose über dem offenen Meer\". Neuronale Netze verarbeiten Informationen zwar ganz anders als wir, dennoch müssen auch sie Lernen, welche Informationen wichtiger sind als andere.\n","\n","Zur Errinnerung, hier das stark vereinfachte Diagram, was das innere (UNet) unseres neuronalen Netzwerks beschreibt:\n","\n","![image](https://drive.google.com/uc?export=view&id=1AiocI2fC6rxr3XusXTOBjA3IrE0XxzJU)"],"metadata":{"id":"pzFstEsZlHG_"}},{"cell_type":"markdown","source":["#### <font color=\"blue\">Frage: Welche Abfolge von dimensionen hälst du im Inneren unseres UNets für angemessen?</font>"],"metadata":{"id":"MkTtxrRRnAXS"}},{"cell_type":"markdown","source":["<font color=\"blue\">*Die input dimensionen sind 64x64. Wir wollen diese dimensionen schrumpfen lassen und dann wieder bei 64 herauskommen. Deshalb bietet sich eine Abfolge von 64 -> 32 -> 24 -> 16 -> 24 -> 32 -> 64 an.*\n","*Die kleinste Stelle im UNet muss immernoch hinreichend groß sein, um alle wichtigen Informationen des Input Bilds zu erfassen. Wir können beispielsweise nicht auf ein 1x1 Bild heruntergehen, denn ein einziger Pixel ist selbstverständlich zu wenig um den Inhalt eines Bilds zu beschreiben. Wie tief man gehen kann ist ein Erfahrungswert, der durch trial and error erlangt wird.*\n","\n","*Diese Zahlen müssen nicht zwangsläufig Vielfache von 2 sein. Theoretisch könnte man auch 64 -> 30 -> 20 -> 10 -> 20 ->30 -> 64 benutzen. Allerdings haben Vielfache (insbesondere Potenzen) von 2 zahlreiche Vorteile in Datenverarbeitungen.*\n","</font>"],"metadata":{"id":"D3Wwe7z2nJ55"}},{"cell_type":"markdown","source":["#### Das Training:\n","Das ausführen des folgenden codeblocks definiert das Model, und startet das Training. Dies kann mehrere Minuten dauern. Wir haben versucht das Training soweit wie möglich zu optimieren. Das führt allerdings zu suboptimalen Ergebnissen. Wenn du die Grenzen des Models austesten möchtest, dann setze oben die IMG_SIZE auf 256, die UNET_SHAPE auf [96, 128, 192] und EPOCHS auf mindestens 30\n","\n","Während das Training läuft werden nach jeder Epoche ein paar Beispielbilder angezeigt. Diese zeigen, was das Model zu der jeweiligen Epoche kann. Du solltest beobachten können, wie die Bilder von Epoche zu Epoche besser werden.\n","\n","Damit du nicht zu lange wartest, solltest du ruhig schonmal zu dem Abschnitt \"Ergebnisse\" vorspringen. Das gesamte Training sollte ca. 5-10 minuten dauern."],"metadata":{"id":"pwHDSuK1o2ht"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nKDiqwhhRBL4"},"outputs":[],"source":["UNET_SHAPE = [16, 24, 32]\n","BLOCK_DEPTH = 2\n","EPOCHS = 10\n","\n","model = DiffusionModel(IMG_SIZE, UNET_SHAPE, BLOCK_DEPTH, BATCH_SIZE)\n","model.compile(optimizer=tf.keras.optimizers.Adam(\n","    learning_rate=1e-3\n","), loss=keras.losses.mean_absolute_error )\n","\n","model.fit(training_set, epochs=EPOCHS, callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images)])"]},{"cell_type":"markdown","source":["##Ergebnisse"],"metadata":{"id":"oJ59St-mrCf9"}},{"cell_type":"markdown","source":["Während das Model trainiert wollen wir hier ein paar Interessante Ergebnisse ansprechen. im oberen Beispiel wird für 10 Epochen trainiert. Zur Errinnerung: Eine Epoche bedeutet, dass alle Bilder im Trainingsdatenset einmal trainiert wurden. Im regelfall werden Modelle für deutlich mehr als 10 Epochen trainiert. Hier sind deshalb ein paar Beispiele nach 50 Epochen des oben definierten Modells:\n","\n","![picture](https://drive.google.com/uc?export=view&id=1-QmnJaBKi-ODG9Imij9TutGQQVxdO9OG)"],"metadata":{"id":"3ehuj0jDttjt"}},{"cell_type":"markdown","source":["Lass uns das damit Vergleichen, was tatsächlich als trainingsinput verwendet wurde. Denn wir errinnern uns: Wir haben die Daten vorverarbeitet (Deutlich runterskaliert auf 64x64)"],"metadata":{"id":"bad059FexAlB"}},{"cell_type":"code","source":["sample_set = dataset.map(format_image, num_parallel_calls=tf.data.AUTOTUNE)\n","sample_images = sample_set.take(50)\n","plt.rcParams[\"figure.figsize\"] = [30, 15]\n","plt.rcParams[\"figure.autolayout\"] = True\n","ctr = 0\n","for row in range(3):\n","  for col in range(6):\n","    index = row * 6 + col\n","    image = list(sample_images.as_numpy_iterator())[index]\n","    #image = data[\"image\"]\n","    #image = image.numpy()\n","    plt.subplot(3, 6, index + 1)\n","    plt.axis(\"off\")\n","    plt.imshow(image, cmap=plt.cm.binary)"],"metadata":{"id":"iwo-nvqLxcxv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Das Ergebnis unseres Trainings ist hiervon weit Entfernt."],"metadata":{"id":"5jyffRVAyN5V"}},{"cell_type":"markdown","source":["#### <font color=\"blue\">Frage: Wie würdest du die Unterschiede beschreiben?</font>"],"metadata":{"id":"tnJy86qCyZvd"}},{"cell_type":"markdown","source":["<font color=\"blue\">*Die echten Bilder haben wesentlich mehr Details. Das Netz hat lediglich gelernt, dass \"Blüten\" ein farbiger Blob auf einem grünen Hintergrund sind.*</font>"],"metadata":{"id":"Lk_H8LEczCXD"}},{"cell_type":"markdown","source":["#### <font color=\"blue\">Frage: Wie könntest du dir die Unterschiede Erklären?</font>"],"metadata":{"id":"MFjTdwyPzTgZ"}},{"cell_type":"markdown","source":["<font color=\"blue\">*Antwort: Ein Mangel an Details in generativen Netzwerken deutet oft auf fehlende Mächtigkeit des Models hin. Dem Model gelingt es grob die Struktur (farbiger Blob auf grünem Hintegrund) zu Lernen. Es hat auch verstanden, dass die meisten Blüten sich aus wenigen Farben zusammensetzen. Es hat aber nicht genug \"Power\" um Beispielsweise verschiedene Formen von Blütenblättern zu verstehen.*\n","</font>"],"metadata":{"id":"8EOhsKbRzuMP"}},{"cell_type":"markdown","source":["### Vortrainierte Version\n","Zu wenig Power? Na dann, machen wir unser Netzwerk größer!\n","\n","Ich habe im Vorfeld des Workshops eine Version des Netzwerks auf unserem AG Server trainiert. Dieses nimmt 64x64 Bilder, und hat deutlich mehr Parameter (Unet Shape 64 -> 128 -> 96 -> 64 -> 32 -> 64 -> 96 -> 128 -> 64)\n","Nach 50 Epochen haben wir folgende Ergebnisse:\n","\n","![image](https://drive.google.com/uc?export=view&id=1-on2DtyTxzOwJPkGI0qQYLUvI45PnVz_)\n","\n","Das sieht doch schon viel mehr nach Blüten aus!\n","Es lassen sich einzelne Blütenblätter erkennen, der Hintergrund ist detaillierter, und die Blüten haben realistische Farbverläufe.\n","Einige Blüten haben aber dennoch merkwürdige, nicht ganz naturgemäße Formen :-)"],"metadata":{"id":"0rP3Wzo_0Ovt"}},{"cell_type":"markdown","source":["Lass uns nun einen genaueren Blick auf die Generation dieser Bilder werfen. Wie in der Einführung besprochen, ist dieses Model ein iteratives Model. Ein Bild (Am Anfang buntes rauschen) wird in das Model gefüttert. Dieses versucht ein klein wenig rauschen zu entfernen, und gibt ein neues Bild aus. Dieses Bild wird dann wieder in das Model gefüttert. Hier eine visualisierung des Prozesses als .gif animiert, und als einzelne Schritte:\n","\n","![image](https://drive.google.com/uc?export=view&id=1HMrcVuuJvbTTXRSChBPbvIjcs1f5nSeb)\n","![image](https://drive.google.com/uc?export=view&id=1-qaFvDfygCAXXTPv9SiFkVI_cw5PSmcY)"],"metadata":{"id":"OhwGor8O3Hog"}},{"cell_type":"markdown","source":["#### <font color=\"blue\">Frage: Wie würdest du den Prozess beschreiben?</font>"],"metadata":{"id":"79W7r0Mf5VM5"}},{"cell_type":"markdown","source":["<font color=\"blue\">*Antwort: Das Netz scheint innerhalb der ersten durchläufe eine Farbe und grobe Form festzulegen. Später kommen dann Details dazu. Es fällt auf, dass die ersten Schritte deutlich mehr Änderungen vornehmen. Vergleiche hier den unterschied von Schritt 1 -> 2 im vergleich zu Schritt 25 -> 26*\n","</font>"],"metadata":{"id":"muQ303Pr5hTc"}},{"cell_type":"markdown","source":["### Quellen\n","\n","Der hier verwendete code stammt von einem [Keras Guide](https://keras.io/examples/generative/ddim/) und wurde von mir zu Zwecken dieses Workshops angepasst. Alle Bilder wurden auf unserem AG Server generiert, Diagramme stammen von mir."],"metadata":{"id":"_2s-Y4vuXvm4"}}],"metadata":{"colab":{"collapsed_sections":["RkN7XhBagVd6","Tzk8bTUFgSYK","HylGFQ8uiRB3","5pBeGOsfjWfD","MkTtxrRRnAXS","tnJy86qCyZvd","MFjTdwyPzTgZ","79W7r0Mf5VM5"],"provenance":[{"file_id":"https://github.com/Jackilion/flower-ddim/blob/master/DDIM.ipynb","timestamp":1664808857269}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}